{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the med-flamingo repository from GitHub\n",
    "#!git clone https://github.com/Abir196/med-flamingo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "# Define the path to the requirements.txt file for the med-flamingo project\n",
    "requirements_path = \"content/med-flamingo/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "old_forward = LlamaForCausalLM.forward\n",
    "\n",
    "def forward(self, input_ids, attention_mask, **kwargs):\n",
    "    \"\"\"Condition the Flamingo layers on the media locations before forward()\"\"\"\n",
    "    if not self.initialized_flamingo:\n",
    "        raise ValueError(\n",
    "            \"Flamingo layers are not initialized. Please call `init_flamingo` first.\"\n",
    "        )\n",
    "\n",
    "    media_locations = input_ids == self.media_token_id\n",
    "\n",
    "    # if there are media already cached and we're generating and there are no media tokens in the input,\n",
    "    # we'll assume that ALL input tokens should attend to the last previous media that is cached.\n",
    "    # this is especially important for HF generate() compatibility, since generate() calls forward()\n",
    "    # repeatedly one token at a time (with no media tokens).\n",
    "    # without this check, the model would not attend to any images when generating (after the first token)\n",
    "    use_cached_media_locations = (\n",
    "        self._use_cached_vision_x\n",
    "        and self.is_conditioned()\n",
    "        and not media_locations.any()\n",
    "    )\n",
    "\n",
    "    for layer in self._get_decoder_layers():\n",
    "        if not use_cached_media_locations:\n",
    "            layer.condition_media_locations(media_locations)\n",
    "        layer.condition_use_cached_media(use_cached_media_locations)\n",
    "\n",
    "    # package arguments for the other parent's forward. since we don't know the order of the arguments,\n",
    "    # make them all kwargs\n",
    "    kwargs[\"input_ids\"] = input_ids\n",
    "    kwargs[\"attention_mask\"] = attention_mask\n",
    "    return old_forward(self, **kwargs)  # Call the other parent's forward method\n",
    "\n",
    "\n",
    "\n",
    "LlamaForCausalLM.forward = forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import open_clip\n",
    "\n",
    "from open_flamingo.src.flamingo import Flamingo\n",
    "from open_flamingo.src.flamingo_lm import FlamingoLMMixin\n",
    "from open_flamingo.src.utils import extend_instance\n",
    "\n",
    "\n",
    "def create_model_and_transforms(\n",
    "    clip_vision_encoder_path: str,\n",
    "    clip_vision_encoder_pretrained: str,\n",
    "    lang_encoder_path: str,\n",
    "    tokenizer_path: str,\n",
    "    cross_attn_every_n_layers: int = 1,\n",
    "    use_local_files: bool = False,\n",
    "    decoder_layers_attr_name: str = None,\n",
    "    freeze_lm_embeddings: bool = False,\n",
    "    **flamingo_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize a Flamingo model from a pretrained vision encoder and language encoder.\n",
    "    Appends special tokens to the tokenizer and freezes backbones.\n",
    "\n",
    "    Args:\n",
    "        clip_vision_encoder_path (str): path to pretrained clip model (e.g. \"ViT-B-32\")\n",
    "        clip_vision_encoder_pretrained (str): name of pretraining dataset for clip model (e.g. \"laion2b_s32b_b79k\")\n",
    "        lang_encoder_path (str): path to pretrained language encoder\n",
    "        tokenizer_path (str): path to pretrained tokenizer\n",
    "        cross_attn_every_n_layers (int, optional): determines how often to add a cross-attention layer. Defaults to 1.\n",
    "        use_local_files (bool, optional): whether to use local files. Defaults to False.\n",
    "        decoder_layers_attr_name (str, optional): name of the decoder layers attribute. Defaults to None.\n",
    "    Returns:\n",
    "        Flamingo: Flamingo model from pretrained vision and language encoders\n",
    "        Image processor: Pipeline to preprocess input images\n",
    "        Tokenizer: A tokenizer for the language model\n",
    "    \"\"\"\n",
    "    vision_encoder, _, image_processor = open_clip.create_model_and_transforms(\n",
    "        clip_vision_encoder_path, pretrained=clip_vision_encoder_pretrained\n",
    "    )\n",
    "    # set the vision encoder to output the visual features\n",
    "    vision_encoder.visual.output_tokens = True\n",
    "\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_path,\n",
    "        local_files_only=use_local_files,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    # add Flamingo special tokens to the tokenizer\n",
    "    text_tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": [\"<|endofchunk|>\", \"<image>\"]}\n",
    "    )\n",
    "    if text_tokenizer.pad_token is None:\n",
    "        # Issue: GPT models don't have a pad token, which we use to\n",
    "        # modify labels for the loss.\n",
    "        text_tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "\n",
    "    lang_encoder = AutoModelForCausalLM.from_pretrained(\n",
    "        lang_encoder_path,\n",
    "        local_files_only=use_local_files,\n",
    "        trust_remote_code=True,\n",
    "        load_in_4bit = True\n",
    "    )\n",
    "\n",
    "    # convert LM to FlamingoLM\n",
    "    extend_instance(lang_encoder, FlamingoLMMixin)\n",
    "\n",
    "    if decoder_layers_attr_name is None:\n",
    "        decoder_layers_attr_name = _infer_decoder_layers_attr_name(lang_encoder)\n",
    "    lang_encoder.set_decoder_layers_attr_name(decoder_layers_attr_name)\n",
    "    lang_encoder.resize_token_embeddings(len(text_tokenizer))\n",
    "\n",
    "    model = Flamingo(\n",
    "        vision_encoder,\n",
    "        lang_encoder,\n",
    "        text_tokenizer.encode(\"<|endofchunk|>\")[-1],\n",
    "        text_tokenizer.encode(\"<image>\")[-1],\n",
    "        vis_dim=open_clip.get_model_config(clip_vision_encoder_path)[\"vision_cfg\"][\n",
    "            \"width\"\n",
    "        ],\n",
    "        cross_attn_every_n_layers=cross_attn_every_n_layers,\n",
    "        **flamingo_kwargs,\n",
    "    )\n",
    "\n",
    "    # Freeze all parameters\n",
    "    model.requires_grad_(False)\n",
    "    assert sum(p.numel() for p in model.parameters() if p.requires_grad) == 0\n",
    "\n",
    "    # Unfreeze perceiver, gated_cross_attn_layers, and LM input embeddings\n",
    "    model.perceiver.requires_grad_(True)\n",
    "    model.lang_encoder.gated_cross_attn_layers.requires_grad_(True)\n",
    "    if not freeze_lm_embeddings:\n",
    "        model.lang_encoder.get_input_embeddings().requires_grad_(True)\n",
    "        # TODO: investigate also training the output embeddings when untied\n",
    "\n",
    "    print(\n",
    "        f\"Flamingo model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\"\n",
    "    )\n",
    "\n",
    "    return model, image_processor, text_tokenizer\n",
    "\n",
    "\n",
    "def _infer_decoder_layers_attr_name(model):\n",
    "    for k in __KNOWN_DECODER_LAYERS_ATTR_NAMES:\n",
    "        if k.lower() in model.__class__.__name__.lower():\n",
    "            return __KNOWN_DECODER_LAYERS_ATTR_NAMES[k]\n",
    "\n",
    "    raise ValueError(\n",
    "        f\"We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.\"\n",
    "    )\n",
    "\n",
    "\n",
    "__KNOWN_DECODER_LAYERS_ATTR_NAMES = {\n",
    "    \"opt\": \"model.decoder.layers\",\n",
    "    \"gptj\": \"transformer.h\",\n",
    "    \"gpt-j\": \"transformer.h\",\n",
    "    \"pythia\": \"gpt_neox.layers\",\n",
    "    \"llama\": \"model.layers\",\n",
    "    \"gptneoxforcausallm\": \"gpt_neox.layers\",\n",
    "    \"mpt\": \"transformer.blocks\",\n",
    "    \"mosaicgpt\": \"transformer.blocks\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Med-Flamingo checkpoint to /home/sd4175/.cache/huggingface/hub/models--med-flamingo--med-flamingo/snapshots/7243cd83bd426ceade9c4de9844cc5e5f3ff75e0/model.pt\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "checkpoint_path = hf_hub_download(\"med-flamingo/med-flamingo\", \"model.pt\")\n",
    "print(f'Downloaded Med-Flamingo checkpoint to {checkpoint_path}')\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.load(checkpoint_path, map_location=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flamingo model initialized with 1309919248 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "import os\n",
    "# from open_flamingo import create_model_and_transforms\n",
    "from accelerate import Accelerator\n",
    "from einops import repeat\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append('/content/med-flamingo/scripts')\n",
    "sys.path.append('/content/med-flamingo')\n",
    "from src.utils import FlamingoProcessor\n",
    "from demo_utils import image_paths, clean_generation\n",
    "\n",
    "accelerator = Accelerator() #when using cpu: cpu=True\n",
    "\n",
    "device = accelerator.device\n",
    "\n",
    "print('Loading model..')\n",
    "\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"huggyllama/llama-7b\",\n",
    "    tokenizer_path= \"huggyllama/llama-7b\",\n",
    "    cross_attn_every_n_layers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.perceiver.cuda()\n",
    "torch.cuda.empty_cache()\n",
    "model.vision_encoder.cuda()\n",
    "torch.cuda.empty_cache()\n",
    "model.lang_encoder.gated_cross_attn_layers.to(torch.float16).cuda()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc  # Import the garbage collection module\n",
    "# Collect and free up unused memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load med-flamingo checkpoint:\n",
    "model.load_state_dict(a, strict=False)\n",
    "processor = FlamingoProcessor(tokenizer, image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sd4175/med-flamingo/src\n"
     ]
    }
   ],
   "source": [
    "cd med-flamingo/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess data\n"
     ]
    }
   ],
   "source": [
    "# Select only the first 2 image paths\n",
    "image_paths = image_paths[:2]\n",
    "\n",
    "# Prepare the model using the Accelerator\n",
    "model = accelerator.prepare(model)\n",
    "is_main_process = accelerator.is_main_process\n",
    "model.eval()\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "# Load demo images using PIL and store in a list\n",
    "demo_images = [Image.open(path) for path in image_paths]\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Define multimodal few-shot prompt\n",
    "\"\"\"\n",
    "\n",
    "# Define a few-shot prompt containing text and <image> placeholders\n",
    "prompt = \"You are a helpful medical assistant. You are being provided with images, a question about the image and an answer. Follow the examples and answer the last question. <image>Question: What is/are the structure near/in the middle of the brain? Answer: pons.<image>Question: Is there evidence of a right apical pneumothorax on this chest x-ray? Answer: yes.<image>Question: Is/Are there air in the patient's peritoneal cavity? Answer: no.<image>Question: Does the heart appear enlarged? Answer: yes.<image>Question: What side are the infarcts located? Answer: bilateral.<image>Question: Which image modality is this? Answer: mr flair.<image>Question: What is the most likely diagnosis?\"\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocess data\n",
    "\"\"\"\n",
    "print('Preprocess data')\n",
    "\n",
    "# Preprocess demo images using the FlamingoProcessor\n",
    "pixels = processor.preprocess_images(demo_images)\n",
    "\n",
    "pixels = repeat(pixels, 'N c h w -> b N T c h w', b=1, T=1)\n",
    "\n",
    "# Encode the text prompt using the FlamingoProcessor\n",
    "tokenized_data = processor.encode_text(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=556x635>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x840>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate from multimodal few-shot prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response=\"<s> You are a helpful medical assistant. You are being provided with images, a question about the image and an answer. Follow the examples and answer the last question. <image> Question: What is/are the structure near/in the middle of the brain? Answer: pons.<image> Question: Is there evidence of a right apical pneumothorax on this chest x-ray? Answer: yes.<image> Question: Is/Are there air in the patient's peritoneal cavity? Answer: no.<image> Question: Does the heart appear enlarged? Answer: yes.<image> Question: What side are the infarcts located? Answer: bilateral.<image> Question: Which image modality is this? Answer: mr flair.<image> Question: What is the most likely diagnosis? Answer: multiple sclerosis.\"\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 4: Generate response\n",
    "\"\"\"\n",
    "\n",
    "# Generate a response using the multimodal few-shot prompt\n",
    "print('Generate from multimodal few-shot prompt')\n",
    "\n",
    "# Use mixed-precision training context for improved performance\n",
    "with torch.autocast('cuda', torch.float16):\n",
    "\n",
    "    # Generate text using the model\n",
    "    generated_text = model.generate(\n",
    "        vision_x=pixels.to(device),  # Convert images to the device\n",
    "        lang_x=tokenized_data[\"input_ids\"].to(device),  # Convert text input to the device\n",
    "        attention_mask=tokenized_data[\"attention_mask\"].to(device),  # Convert attention mask to the device\n",
    "        max_new_tokens=10,  # Limit the maximum number of new tokens in the generated response\n",
    "    )\n",
    "\n",
    "# Decode the generated text using the processor's tokenizer\n",
    "response = processor.tokenizer.decode(generated_text[0])\n",
    "\n",
    "# Clean up the generated response\n",
    "response = clean_generation(response)\n",
    "\n",
    "# Print the cleaned response\n",
    "print(f'{response=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
