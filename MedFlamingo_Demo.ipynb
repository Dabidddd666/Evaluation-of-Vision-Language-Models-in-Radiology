{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# model_path = \"/vast/work/public/ml-datasets/llama-2/Llama-2-7b-hf\"  # Replace with the full path if needed\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModel.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download \n",
    "import torch \n",
    "import os \n",
    "from open_flamingo import create_model_and_transforms \n",
    "from accelerate import Accelerator \n",
    "from einops import repeat\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import FlamingoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_generation(response):\n",
    "    \"\"\"\n",
    "    for some reason, the open-flamingo based model slightly changes the input prompt (e.g. prepends <unk>, an adds some spaces)\n",
    "    \"\"\"\n",
    "    return response.replace('<unk> ', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    'img/synpic50962.jpg',\n",
    "    'img/synpic52767.jpg',\n",
    "    'img/synpic30324.jpg',\n",
    "    'img/synpic21044.jpg',\n",
    "    'img/synpic54802.jpg',\n",
    "    'img/synpic57813.jpg',\n",
    "    'img/synpic47964.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    accelerator = Accelerator() #when using cpu: cpu=True\n",
    "\n",
    "    device = accelerator.device\n",
    "\n",
    "    print('Loading model..')\n",
    "\n",
    "    # >>> add your local path to Llama-7B (v1) model here:\n",
    "    llama_path = \"/vast/work/public/ml-datasets/llama-2/Llama-2-7b-hf\"\n",
    "    if not os.path.exists(llama_path):\n",
    "        raise ValueError('Llama model not yet set up, please check README for instructions!')\n",
    "\n",
    "    model, image_processor, tokenizer = create_model_and_transforms(\n",
    "        clip_vision_encoder_path=\"ViT-L-14\",\n",
    "        clip_vision_encoder_pretrained=\"openai\",\n",
    "        lang_encoder_path=llama_path,\n",
    "        tokenizer_path=llama_path,\n",
    "        cross_attn_every_n_layers=4\n",
    "    )\n",
    "    # load med-flamingo checkpoint:\n",
    "    checkpoint_path = hf_hub_download(\"med-flamingo/med-flamingo\", \"model.pt\")\n",
    "    print(f'Downloaded Med-Flamingo checkpoint to {checkpoint_path}')\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)\n",
    "    processor = FlamingoProcessor(tokenizer, image_processor)\n",
    "\n",
    "    # go into eval model and prepare:\n",
    "    model = accelerator.prepare(model)\n",
    "    is_main_process = accelerator.is_main_process\n",
    "    model.eval()\n",
    "\n",
    "    \"\"\"\n",
    "    Step 1: Load images\n",
    "    \"\"\"\n",
    "    demo_images = [Image.open(path) for path in image_paths]\n",
    "\n",
    "    \"\"\"\n",
    "    Step 2: Define multimodal few-shot prompt \n",
    "    \"\"\"\n",
    "\n",
    "    # example few-shot prompt:\n",
    "    prompt = \"You are a helpful medical assistant. You are being provided with images, a question about the image and an answer. Follow the examples and answer the last question. <image>Question: What is/are the structure near/in the middle of the brain? Answer: pons.<|endofchunk|><image>Question: Is there evidence of a right apical pneumothorax on this chest x-ray? Answer: yes.<|endofchunk|><image>Question: Is/Are there air in the patient's peritoneal cavity? Answer: no.<|endofchunk|><image>Question: Does the heart appear enlarged? Answer: yes.<|endofchunk|><image>Question: What side are the infarcts located? Answer: bilateral.<|endofchunk|><image>Question: Which image modality is this? Answer: mr flair.<|endofchunk|><image>Question: Where is the largest mass located in the cerebellum? Answer:\"\n",
    "\n",
    "    \"\"\"\n",
    "    Step 3: Preprocess data \n",
    "    \"\"\"\n",
    "    print('Preprocess data')\n",
    "    pixels = processor.preprocess_images(demo_images)\n",
    "    pixels = repeat(pixels, 'N c h w -> b N T c h w', b=1, T=1)\n",
    "    tokenized_data = processor.encode_text(prompt)\n",
    "\n",
    "    \"\"\"\n",
    "    Step 4: Generate response \n",
    "    \"\"\"\n",
    "\n",
    "    # actually run few-shot prompt through model:\n",
    "    print('Generate from multimodal few-shot prompt')\n",
    "    generated_text = model.generate(\n",
    "    vision_x=pixels.to(device),\n",
    "    lang_x=tokenized_data[\"input_ids\"].to(device),\n",
    "    attention_mask=tokenized_data[\"attention_mask\"].to(device),\n",
    "    max_new_tokens=10,\n",
    "    )\n",
    "\n",
    "    #free memory\n",
    "    torch.cuda.empty_cache() \n",
    "    response = processor.tokenizer.decode(generated_text[0])\n",
    "    response = clean_generation(response)\n",
    "\n",
    "    print(f'{response=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:24<00:00, 42.31s/it]\n",
      "/ext3/miniconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flamingo model initialized with 1309919248 trainable parameters\n",
      "Downloaded Med-Flamingo checkpoint to /home/sd4175/.cache/huggingface/hub/models--med-flamingo--med-flamingo/snapshots/7243cd83bd426ceade9c4de9844cc5e5f3ff75e0/model.pt\n",
      "Preprocess data\n",
      "Generate from multimodal few-shot prompt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response=\"<s> You are a helpful medical assistant. You are being provided with images, a question about the image and an answer. Follow the examples and answer the last question. <image> Question: What is/are the structure near/in the middle of the brain? Answer: pons.<|endofchunk|><image> Question: Is there evidence of a right apical pneumothorax on this chest x-ray? Answer: yes.<|endofchunk|><image> Question: Is/Are there air in the patient's peritoneal cavity? Answer: no.<|endofchunk|><image> Question: Does the heart appear enlarged? Answer: yes.<|endofchunk|><image> Question: What side are the infarcts located? Answer: bilateral.<|endofchunk|><image> Question: Which image modality is this? Answer: mr flair.<|endofchunk|><image> Question: Where is the largest mass located in the cerebellum? Answer:agi Gewficficficficficibm Gewibm\"\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
